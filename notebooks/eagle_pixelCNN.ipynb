{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab8127",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfh.datasets import setup_environment, tng100, eagle\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b650134",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_eagle = tfds.load('eagle', split='train')\n",
    "dset_tng = tfds.load('tng100', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f719b2f",
   "metadata": {},
   "source": [
    "# Plot some examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(\"Train\",len(dset_eagle))\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "for example in dset_eagle.take(10):\n",
    "    #print((example['wl_sort'])[example['inds_valid']])\n",
    "    time_vec = example['time']\n",
    "    inds_valid = example['inds_valid']\n",
    "    axs.scatter((example['wl_sort'])[example['inds_valid']],np.log10(example['sed']))\n",
    "    axs.set_xscale('log')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "for example in dset_eagle.take(10):\n",
    "    #print(wl[example['inds_valid']])\n",
    "    axs.plot(example['time'],example['SFR_Max'])\n",
    "    #axs.set_xscale('log')    \n",
    "    #sed = (tf.gather(example['sed'],inds, axis=1) + 20.70243)/2.0466275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Train\",len(dset_tng))\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "for example_tng, example_eagle in zip(dset_tng.take(10),dset_eagle.take(10)):\n",
    "    #print(wl[example['inds_valid']])\n",
    "    #print(wl_tng)\n",
    "    #print(example['sed'])\n",
    "    print((example_tng['wl_sort'])[example_eagle['inds_valid']])\n",
    "    axs.scatter((example_tng['wl_sort'])[example_eagle['inds_valid']],np.log10((example_tng['sed'])[example_eagle['inds_valid']]))\n",
    "    axs.set_xscale('log')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "for example in dset_tng.take(10):\n",
    "    #print(wl[example['inds_valid']])\n",
    "    axs.plot(example['time'],example['SFR_Max'])\n",
    "    #axs.set_xscale('log')    \n",
    "    #sed = (tf.gather(example['sed'],inds, axis=1) + 20.70243)/2.0466275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335771",
   "metadata": {},
   "source": [
    "# Define datasets for TNG and EAGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(example):\n",
    "    return tf.reshape(example['SFR_Max'],(-1,100,1)), \\\n",
    "           tf.reshape(example['SFR_Max'],(-1,100,1))\n",
    "\n",
    "def preprocessing_wmass(example):\n",
    "    mass = example['Mstar'][:,0]\n",
    "    mass_half = example['Mstar_Half'][:,0]\n",
    "    tiler = tf.constant([100])\n",
    "    mass = tf.reshape(tf.tile(mass, tiler),(-1,100,1))\n",
    "    mass_half = tf.reshape(tf.tile(mass_half, tiler),(-1,100,1))\n",
    "    sfr = tf.math.add(tf.reshape(example['SFR_Max'],(-1,100,1)), 1e-5)\n",
    "    res = tf.concat([sfr, mass, mass_half], axis=2)\n",
    "    return res, res\n",
    "\n",
    "def preprocessing_wmass_atan(example):\n",
    "    mass = example['Mstar'][:,0]\n",
    "    #mass_half = example['Mstar_Half'][:,0]\n",
    "    #sed = (tf.gather(example['sed'],inds, axis=1) + 20.70243)/2.0466275\n",
    "    sed = example['sed']\n",
    "    tiler = tf.constant([100])\n",
    "    mass = tf.reshape(tf.tile(mass, tiler),(-1,100,1))\n",
    "    #mass_half = tf.reshape(tf.tile(mass_half, tiler),(-1,100,1))\n",
    "    sfr = tf.math.tanh(tf.math.asinh(tf.reshape(example['SFR_Max'],(-1,100,1))/40) + 1e-3 + 0.005*tf.math.softplus(tf.random.normal(shape=[64,100,1])))\n",
    "    res = tf.concat([sfr], axis=2) #  mass, mass_half\n",
    "    return (res, sed), res\n",
    "\n",
    "def input_fn(mode='train', batch_size=64, \n",
    "             dataset_name='tng100', data_dir=None,\n",
    "             include_mass=True, arctan=True):\n",
    "    \"\"\"\n",
    "    mode: 'train' or 'test'\n",
    "    \"\"\"\n",
    "    keys = ['sed','Mstar', 'SFR_Max', 'mass_quantiles', 'sed', 'time']\n",
    "    if mode == 'train':\n",
    "        dataset = tfds.load(dataset_name, split='train[:90%]')\n",
    "        dataset = dataset.map(lambda x: {k:x[k] for k in keys})\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    else:\n",
    "        dataset = tfds.load(dataset_name, split='train[90%:]')\n",
    "        dataset = dataset.map(lambda x: {k:x[k] for k in keys}) #dataset = dataset.repeat()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    if include_mass and arctan:\n",
    "        dataset = dataset.map(preprocessing_wmass_atan) # Apply data preprocessing\n",
    "    elif include_mass:\n",
    "        dataset = dataset.map(preprocessing_wmass)\n",
    "    else : \n",
    "        dataset = dataset.map(preprocessing)\n",
    "    dataset = dataset.prefetch(-1)       # fetch next batches while training current one (-1 for autotune)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(example):\n",
    "    return tf.reshape(example['SFR_Max'],(-1,100,1)), \\\n",
    "           tf.reshape(example['SFR_Max'],(-1,100,1))\n",
    "\n",
    "def preprocessing_wmass(example):\n",
    "    mass = example['Mstar'][:,0]\n",
    "    mass_half = example['Mstar_Half'][:,0]\n",
    "    tiler = tf.constant([100])\n",
    "    mass = tf.reshape(tf.tile(mass, tiler),(-1,100,1))\n",
    "    mass_half = tf.reshape(tf.tile(mass_half, tiler),(-1,100,1))\n",
    "    sfr = tf.math.add(tf.reshape(example['SFR_Max'],(-1,100,1)), 1e-5)\n",
    "    res = tf.concat([sfr, mass, mass_half], axis=2)\n",
    "    return res, res\n",
    "\n",
    "def preprocessing_wmass_atan_tng(example):\n",
    "    mass = example['Mstar'][:,0]\n",
    "    #mass_half = example['Mstar_Half'][:,0]\n",
    "    sed = (tf.gather(example['sed'],np.squeeze(np.where(inds_valid)), axis=1))\n",
    "    #sed = example['sed']\n",
    "    tiler = tf.constant([100])\n",
    "    mass = tf.reshape(tf.tile(mass, tiler),(-1,100,1))\n",
    "    #mass_half = tf.reshape(tf.tile(mass_half, tiler),(-1,100,1))\n",
    "    sfr = tf.math.tanh(tf.math.asinh(tf.reshape(example['SFR_Max'],(-1,100,1))/40) + 1e-3 + 0.005*tf.math.softplus(tf.random.normal(shape=[64,100,1])))\n",
    "    res = tf.concat([sfr], axis=2) #  mass, mass_half\n",
    "    return (res, sed), res\n",
    "\n",
    "def input_fn_tng(mode='train', batch_size=64, \n",
    "             dataset_name='tng100', data_dir=None,\n",
    "             include_mass=True, arctan=True):\n",
    "    \"\"\"\n",
    "    mode: 'train' or 'test'\n",
    "    \"\"\"\n",
    "    keys = ['sed','Mstar', 'SFR_Max', 'mass_quantiles', 'sed', 'time']\n",
    "    if mode == 'train':\n",
    "        dataset = tfds.load(dataset_name, split='train[:90%]')\n",
    "        dataset = dataset.map(lambda x: {k:x[k] for k in keys})\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    else:\n",
    "        dataset = tfds.load(dataset_name, split='train[90%:]')\n",
    "        dataset = dataset.map(lambda x: {k:x[k] for k in keys}) #dataset = dataset.repeat()\n",
    "        \n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    if include_mass and arctan:\n",
    "        dataset = dataset.map(preprocessing_wmass_atan_tng) # Apply data preprocessing\n",
    "    elif include_mass:\n",
    "        dataset = dataset.map(preprocessing_wmass)\n",
    "    else : \n",
    "        dataset = dataset.map(preprocessing)\n",
    "    dataset = dataset.prefetch(-1)       # fetch next batches while training current one (-1 for autotune)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "dtrain_eagle = input_fn(mode='train', batch_size=batch_size, dataset_name='eagle')\n",
    "dval_eagle = input_fn(mode='val', batch_size=batch_size, dataset_name='eagle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "dtrain_tng = input_fn_tng(mode='train', batch_size=batch_size, dataset_name='tng100')\n",
    "dval_tng = input_fn_tng(mode='val', batch_size=batch_size, dataset_name='tng100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9acbb3",
   "metadata": {},
   "source": [
    "# Train on TNG100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Keras model implementing PixelCNN.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n",
    "import time\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfkl=keras.layers\n",
    "\n",
    "def generate_model(n_timesteps, n_filters, *, n_channels=1, n_components=2, kernel_size=3,\n",
    "                   n_dilations=5, list_of_dilation_rates=None,\n",
    "                   list_of_filters=None):\n",
    "    \"\"\"Generate the PixelCNN Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_timesteps : int\n",
    "        Number of time steps.\n",
    "    n_filters : int\n",
    "        Number of filters.\n",
    "    n_channels : int, default 1\n",
    "        Number of channels in the dataset\n",
    "    n_components : int, default 2\n",
    "        Number of components in the Gaussian mixture distribution.\n",
    "    kernel_size : int, default 3\n",
    "        Size of the convolution kernel.\n",
    "    n_dilations : int, default 5\n",
    "        Number of dilated convolutions to do. For each convolution, the\n",
    "        dilation rate is 2**idx+1 and the number of filters is 2**idx+4.\n",
    "    list_of_dilation_rates : list of int or None, default None\n",
    "        List of the dilation rates to use in the dilated convolutions. If not\n",
    "        None, the n_dilations is not used and filters must be given with the\n",
    "        same size.\n",
    "    list_of_filters : list of int or None, default None\n",
    "        List of the filter number for each of the dilated convolutions. Must be\n",
    "        of the same size as list_of_dilation_rates\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Keras model\n",
    "\n",
    "    \"\"\"\n",
    "    # Shape of the distribution\n",
    "    event_shape = [1]\n",
    "    # Compute how many parameters this distribution requires\n",
    "    params_size = 2\n",
    "    #print(params_size)\n",
    "\n",
    "    \n",
    "    input_sfh = keras.layers.Input(shape=(n_timesteps,1))\n",
    "    input_sed = keras.layers.Input(shape=(n_filters,1))\n",
    "    \n",
    "    # Compress the SED and return some channels\n",
    "    sed_net = tf.keras.Sequential([\n",
    "        tfkl.Input(shape=(125, 1)),\n",
    "        tfkl.Conv1D(16, 3, strides=2, padding='same', activation='relu'),\n",
    "        tfkl.Conv1D(32, 3, strides=2, padding='same', activation='relu'),\n",
    "        tfkl.Conv1D(64, 3, strides=2, padding='same', activation='relu'),\n",
    "        tfkl.Conv1D(64, 3, strides=1, padding='same', activation='relu'),\n",
    "        tfkl.Flatten(),\n",
    "        tfkl.Dense(128, activation='relu'),\n",
    "        tfkl.Dense(8, activation='softplus'),\n",
    "        tfkl.Lambda(lambda x: tf.tile(tf.reshape(x,[-1,1,8]), [1,100,1]))\n",
    "        ])\n",
    "    \n",
    "    merged = keras.layers.Concatenate(axis=-1)([input_sfh, \n",
    "                                                sed_net(input_sed)])\n",
    "    \n",
    "    \n",
    "    # Shift and cut\n",
    "    net = keras.layers.Lambda(\n",
    "            lambda x: tf.pad(x, paddings=tf.constant([[0, 0], [1, 0], [0, 0]]))\n",
    "        )(merged)\n",
    "    \n",
    "    net=keras.layers.Lambda(\n",
    "            lambda x: x[:, :-1, :]\n",
    "        )(net)\n",
    "    \n",
    "\n",
    "    net=keras.layers.Conv1D(\n",
    "            filters=16,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=1,\n",
    "            padding='causal',\n",
    "            activation='relu'\n",
    "        )(net)\n",
    "\n",
    "    if list_of_dilation_rates is None:\n",
    "        list_of_dilation_rates = [2**(i+1) for i in range(n_dilations)]\n",
    "        list_of_filters = [2**(i+4) for i in range(n_dilations)]\n",
    "    elif len(list_of_filters) != len(list_of_dilation_rates):\n",
    "        raise ValueError(\n",
    "            \"filters and list_of_dilation_rates must have the same length\")\n",
    "\n",
    "    for dilation_rate, nb_filters in zip(list_of_dilation_rates,\n",
    "                                         list_of_filters):\n",
    "        net = keras.layers.Conv1D(\n",
    "                filters=nb_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation_rate=dilation_rate,\n",
    "                padding='causal',\n",
    "                activation='relu')(net)\n",
    "    \n",
    "    net = keras.layers.Dense(2)(net)\n",
    "    \n",
    "    net = tfp.layers.DistributionLambda(\n",
    "                    make_distribution_fn=lambda t: tfd.Beta(\n",
    "                          concentration1=tf.math.softplus(t[..., 0])+1e-3,\n",
    "                          concentration0=tf.math.softplus(t[..., 1])+1e-3)\n",
    "                    )(net)\n",
    "    \n",
    "    pixel_cnn = keras.models.Model(inputs=[input_sfh, input_sed],\n",
    "                                  outputs=net)\n",
    "\n",
    "    # Use the negative log-likelihood as loss function.\n",
    "    def negloglik(y, q):\n",
    "        return tf.reduce_sum(-q.log_prob(y[...,0]), -1)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    pixel_cnn.compile(loss=negloglik, optimizer=opt)\n",
    "\n",
    "    return pixel_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn = generate_model(100,125)\n",
    "\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pixel_cnn.fit(dtrain_tng, \n",
    "                     epochs=epochs,\n",
    "                     steps_per_epoch=1000,validation_data=dval_tng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c06b3c",
   "metadata": {},
   "source": [
    "# Test on TNG100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_test = dval_tng.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=15\n",
    "sample = np.zeros([64,100,1])\n",
    "true = data[0][0][ind,:,0]\n",
    "sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "# init at the \n",
    "sample[:,0,0] = true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3abc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    tmp = pixel_cnn((sample, sed)).sample()\n",
    "    sample[:,i+1,0] = tmp[:,i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d316c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true,label='true SFH')\n",
    "for i in range(64):\n",
    "    plt.plot(sample[i,:,0],color='C1',alpha=0.1)\n",
    "plt.plot(sample[1,:,0],color='C1',alpha=1.,label='individual sample')    \n",
    "plt.plot(sample.mean(axis=0)[:,0],'--',color='red',label='mean posterior')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b2749",
   "metadata": {},
   "source": [
    "# check summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410037fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def find_summaries(mass, time, percentiles=np.linspace(0.1, 0.9, 9)):\n",
    "\n",
    "    ''' compute the half mass and the half time of a galaxy \n",
    "          Input: \n",
    "                - mass: array. The mass history of the galaxy.\n",
    "                - time: array. The corresponding time for the galaxy history.\n",
    "                - percentiles: array. The summaries you want to predict by default 0.1, 0.2,..., 0.9. \n",
    "          Output: the time of the summaries, the corresponding masses, and the index of the mass/time summary.\n",
    "    '''\n",
    "\n",
    "    summary_masses = []\n",
    "    summary_times = []\n",
    "    summary_indices = []\n",
    "    for percentile in percentiles:\n",
    "        summary_mass = min(mass, key=lambda x: abs(x-mass[0]*percentile))  # find mass closest to the half mass\n",
    "        #pdb.set_trace()\n",
    "        summary_masses.append(summary_mass)\n",
    "        summary_mass_indices = np.where(mass == summary_mass)[0]  # find the corresponding indices\n",
    "        summary_mass_index = summary_mass_indices[0]  # chose the first index for the half mass\n",
    "        summary_indices.append(summary_mass_index)\n",
    "        summary_time = time[summary_mass_index]  # find the corresponding half time\n",
    "        summary_times.append(summary_time)\n",
    "\n",
    "    return np.array(summary_times).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c37ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = (tbins[1:] + tbins[:-1] )/2.\n",
    "deltat=time_vec[1:] - time_vec[:-1]\n",
    "#print(deltat)\n",
    "mgrowth_true = np.cumsum(deltat*np.flip(true[1:]))\n",
    "mgrowth_pred = np.cumsum(deltat*np.flip(sample[18,1:,0]))\n",
    "\n",
    "plt.plot(np.flip(time_vec[1:]),mgrowth_true,color='blue',label=\"true\")\n",
    "plt.plot(np.flip(time_vec[1:]),mgrowth_pred,color='red',label='pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc691726",
   "metadata": {},
   "outputs": [],
   "source": [
    "t50_pred=[]\n",
    "t50_true=[]\n",
    "for j in range(1):\n",
    "    try:\n",
    "        data = next(dset_test)\n",
    "        for ind in range(64):\n",
    "            sample = np.zeros([64,100,1])\n",
    "            true = data[0][0][ind,:,0]\n",
    "            sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "            # init at the \n",
    "            sample[:,0,0] = true[0]\n",
    "            for i in range(99):\n",
    "                tmp = pixel_cnn((sample, sed)).sample()\n",
    "                sample[:,i+1,0] = tmp[:,i+1]\n",
    "            mgrowth_true = np.cumsum(deltat*np.flip(true[1:]))\n",
    "            mgrowth_pred = np.cumsum(deltat*np.flip(sample[18,1:,0]))\n",
    "            #pdb.set_trace()\n",
    "            summ_true = find_summaries(np.flip(mgrowth_true), np.flip(time_vec))\n",
    "            summ_pred = find_summaries(np.flip(mgrowth_pred), np.flip(time_vec))\n",
    "            t50_true.append(summ_true[4])\n",
    "            t50_pred.append(summ_pred[4])\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t50_true,t50_pred)\n",
    "plt.plot(np.linspace(40,90,100),np.linspace(40,90,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ba0fd",
   "metadata": {},
   "source": [
    "# Test on EAGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f15b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_eagle = dval_eagle.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51181a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dset_eagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=19\n",
    "sample = np.zeros([64,100,1])\n",
    "true = data[0][0][ind,:,0]\n",
    "sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "# init at the \n",
    "sample[:,0,0] = true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dfd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    tmp = pixel_cnn((sample, sed)).sample()\n",
    "    sample[:,i+1,0] = tmp[:,i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true,label='true SFH')\n",
    "for i in range(64):\n",
    "    plt.plot(sample[i,:,0],color='C1',alpha=0.1)\n",
    "plt.plot(sample[1,:,0],color='C1',alpha=1.,label='individual sample')    \n",
    "plt.plot(sample.mean(axis=0)[:,0],'--',color='red',label='mean posterior')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d306c",
   "metadata": {},
   "source": [
    "# check summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (time_vec[1:] + time_vec[:-1] )/2.\n",
    "print(t.shape)\n",
    "print(t)\n",
    "print(true.shape)\n",
    "deltat=time_vec[1:] - time_vec[:-1]\n",
    "print(deltat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64722fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = (tbins[1:] + tbins[:-1] )/2.\n",
    "deltat=time_vec[1:] - time_vec[:-1]\n",
    "print(deltat)\n",
    "mgrowth_true = np.cumsum(deltat*true[1:])\n",
    "mgrowth_pred = np.cumsum(deltat*sample.mean(axis=0)[1:,0])\n",
    "\n",
    "plt.plot(np.flip(time_vec[1:]),mgrowth_true,color='blue',label=\"true\")\n",
    "plt.plot(np.flip(time_vec[1:]),mgrowth_pred,color='red',label='pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_true = find_summaries(np.flip(mgrowth_true), np.flip(time_vec))\n",
    "summ_pred = find_summaries(np.flip(mgrowth_pred), np.flip(time_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade983e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summ_true)\n",
    "print(summ_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3020dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t50_pred=[]\n",
    "t50_true=[]\n",
    "for j in range(1):\n",
    "    try:\n",
    "        data = next(dset_eagle)\n",
    "        for ind in range(64):\n",
    "            sample = np.zeros([64,100,1])\n",
    "            true = data[0][0][ind,:,0]\n",
    "            sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "            # init at the \n",
    "            sample[:,0,0] = true[0]\n",
    "            for i in range(99):\n",
    "                tmp = pixel_cnn((sample, sed)).sample()\n",
    "                sample[:,i+1,0] = tmp[:,i+1]\n",
    "            mgrowth_true = np.cumsum(deltat*np.flip(true[1:]))\n",
    "            mgrowth_pred = np.cumsum(deltat*np.flip(sample[18,1:,0]))\n",
    "            #pdb.set_trace()\n",
    "            summ_true = find_summaries(np.flip(mgrowth_true), np.flip(time_vec))\n",
    "            summ_pred = find_summaries(np.flip(mgrowth_pred), np.flip(time_vec))\n",
    "            t50_true.append(summ_true[4])\n",
    "            t50_pred.append(summ_pred[4])\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t50_true,t50_pred)\n",
    "plt.plot(np.linspace(40,90,100),np.linspace(40,90,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda0cd4",
   "metadata": {},
   "source": [
    "# train on Eagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn = generate_model(100,125)\n",
    "\n",
    "pixel_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94820424",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pixel_cnn.fit(dtrain_eagle, \n",
    "                     epochs=epochs,\n",
    "                     steps_per_epoch=1000,validation_data=dval_eagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61243cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_eagle = dval_eagle.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dset_eagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed57d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=19\n",
    "sample = np.zeros([64,100,1])\n",
    "true = data[0][0][ind,:,0]\n",
    "sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "# init at the \n",
    "sample[:,0,0] = true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=19\n",
    "sample = np.zeros([64,100,1])\n",
    "true = data[0][0][ind,:,0]\n",
    "sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "# init at the \n",
    "sample[:,0,0] = true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    tmp = pixel_cnn((sample, sed)).sample()\n",
    "    sample[:,i+1,0] = tmp[:,i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71253677",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true,label='true SFH')\n",
    "for i in range(64):\n",
    "    plt.plot(sample[i,:,0],color='C1',alpha=0.1)\n",
    "plt.plot(sample[1,:,0],color='C1',alpha=1.,label='individual sample')    \n",
    "plt.plot(sample.mean(axis=0)[:,0],'--',color='red',label='mean posterior')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t50_pred=[]\n",
    "t50_true=[]\n",
    "for j in range(1):\n",
    "    try:\n",
    "        data = next(dset_eagle)\n",
    "        for ind in range(64):\n",
    "            sample = np.zeros([64,100,1])\n",
    "            true = data[0][0][ind,:,0]\n",
    "            sed = data[0][1][ind].reshape([1,125,1]).repeat(64,axis=0)\n",
    "            # init at the \n",
    "            sample[:,0,0] = true[0]\n",
    "            for i in range(99):\n",
    "                tmp = pixel_cnn((sample, sed)).sample()\n",
    "                sample[:,i+1,0] = tmp[:,i+1]\n",
    "            mgrowth_true = np.cumsum(deltat*np.flip(true[1:]))\n",
    "            mgrowth_pred = np.cumsum(deltat*np.flip(sample[18,1:,0]))\n",
    "            #pdb.set_trace()\n",
    "            summ_true = find_summaries(np.flip(mgrowth_true), np.flip(time_vec))\n",
    "            summ_pred = find_summaries(np.flip(mgrowth_pred), np.flip(time_vec))\n",
    "            t50_true.append(summ_true[4])\n",
    "            t50_pred.append(summ_pred[4])\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd432de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t50_true,t50_pred)\n",
    "plt.plot(np.linspace(40,90,100),np.linspace(40,90,100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
